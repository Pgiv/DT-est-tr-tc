# Comparaison des différentes méthodes {#sec-comparison}

Les différentes méthode de construction de moyennes mobiles asymétriques sont comparées sur des données simulées et des données réelles. 
Pour toutes les séries, un filtre symétrique de 13 termes est utilisé.
Ces méthodes sont également comparées aux estimations obtenues en prolongeant la série par un modèle ARIMA^[Le modèle ARIMA est déterminé automatiquement en n'utilisant pas de retard saisonnier (les séries étant désaisonnalisées) et en n'utilisant aucune variable extérieure (comme des régresseurs externes pour la correction des points atypiques).] puis en appliquant un filtre symétrique de Henderson de 13 termes.

## Méthodologie

En suivant une méthodologie proche de celle de @DarneDagum2009, neuf séries mensuelles sont simulées entre janvier 1960 et décembre 2020 avec différent niveaux de variabilité. Chaque série simulée $y_t= C_t+ T_t + I_t$ peut s'écrire comme la somme de trois composantes :

- le cycle $C_t = \rho [\cos (2 \pi t / \lambda) +\sin (2 \pi t / \lambda)]$, $\lambda$ est fixé à 72 (cycles de 6 ans, il y a donc 19 points de retournement détectables) ;

- la tendance $T_t = T_{t-1} + \nu_t$ avec $\nu_t \sim \mathcal{N}(0, \sigma_\nu^2)$, $\sigma_\nu$ étant fixé à $0,08$ ;

- et l'irrégulier $I_t = e_t$ avec $e_t \sim \mathcal{N}(0, \sigma_e^2)$.

Pour les différentes simulations, nous faisons varier les paramètre $\rho$ et $\sigma_e^2$ afin d'avoir d'avoir des séries avec différents rapports signal sur bruit (voir graphique \@ref(fig:graphs-data-simul)) :

- Fort rapport signal sur bruit (c'est-à-dire un I-C ratio faible et une faible variabilité) : $\sigma_e^2=0,2$ et $\rho = 3,0,\, 3,5$ ou $4,0$ (I-C ratio compris entre 0,9 et 0,7) ;

- Rapport signal sur bruit moyen (c'est-à-dire un I-C ratio moyen et une variabilité moyenne) : $\sigma_e^2=0,3$ et $\rho = 1,5,\, 2,0$ ou $3,0$ (I-C ratio compris entre 2,3 et 1,4) ;

- Faible rapport signal sur bruit (c'est-à-dire un I-C ratio fort et une forte variabilité) : $\sigma_e^2=0,4$ et $\rho = 0,5,\, 0,7$ ou $1,0$ (I-C ratio compris entre 8,9 et 5,2).


```{r graphs-data-simul, echo=FALSE, out.width="100%", fig.cap="Séries simulées à variabilité faible ($\\sigma_e^2=0,2$ et $\\rho = 3,5$), moyenne ($\\sigma_e^2=0,3$ et $\\rho = 2,0$) et forte ($\\sigma_e^2=0,4$ et $\\rho = 1,0$)."}
img <- sprintf("img/simulations/simul_data.%s", fig.ext)
knitr::include_graphics(img)
```

Pour chaque série et chaque date, la tendance-cycle est estimée en utilisant les différentes méthodes présentées dans de ce rapport.
Pour les régressions polynomiales locales, les filtres asymétriques sont calibrés en utilisant l'I-C ratio estimé à chaque date (en appliquant un filtre de Henderson de 13 termes) et pour la méthode FST, un quadrillage du plan est réalisé avec un pas de $0,05$ et avec comme contraintes linéaires la préservation des polynômes de degrés 0 à 3.
Trois critères de qualité sont également calculés :

1. Calcul du déphasage dans la détection des points de retournement. La définition @Zellner1991 est utilisée pour déterminer les points de retournement :  
    - on parle de redressement (*upturn*) lorsque l'on passe d'une phase de récession à une phase d'expansion de l'économie. 
    C'est le cas à la date $t$ lorsque $y_{t-3}\geq y_{t-2}\geq y_{t-1}<y_t\leq y_{t+1}$.  
    - on parle de ralentissement (*downturn*) lorsque l'on passe d'une phase d'expansion à une phase de récession.
C'est le cas à la date $t$ lorsque $y_{t-3}\leq y_{t-2}\leq y_{t-1}>y_t\geq y_{t+1}$.  

    Le déphasage est souvent définit comme le nombre de mois nécessaires pour détecter le bon point de retournement (i.e., le point de retournement sur la composante cyclique).
    Nous utilisons ici un critère légèrement modifié : le déphasage est définit comme le nombre de mois nécessaires pour détecter le bon point de retournement sans aucune révision future.
    Il peut en effet arriver que le bon point de retournement soit détecté par des filtres asymétriques mais ne le soit pas avec l'estimation finale avec un filtre symétrique (c'est le cas de 41 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave) ou qu'il y ait des révisions dans les estimations successives (c'est le cas de 7 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave).
    Finalement, relativement peu de points de retournement sont détectés à la bonne date avec l'estimation finale.
    Avec le filtre de Henderson de 13 termes, 18 sont correctement détectés sur les séries avec une faible variabilité (sur les 57 possibles), 11 sur les séries à variabilité moyenne et 12 sur les séries à forte variabilité.

2. Calcul de deux critères de révisions : la moyenne des écarts relatifs entre la $q$\ieme{} estimation et la dernière estimation $MAE_{fe}(q)$ et la moyenne des écarts relatifs entre la $q$\ieme{} et la $q+1$\ieme{} estimation $MAE_{qe}(q)$
$$
MAE_{fe}(q)=\mathbb E\left[
\left|\frac{
y_{t|t+q} -  y_{t|last}
}{
 y_{t|last}
}\right|
\right]
\quad\text{et}\quad
MAE_{qe}(q)=\mathbb E\left[
\left|\frac{
y_{t|t+q} - y_{t|t+q+1}
}{
y_{t|t+q+1}
}\right|
\right]
$$

## Séries simulées

### Comparaison des filtres polynomiaux locaux et des filtres RKHS

Par simplification, pour l'approche polynomiale locale, nous ne présenterons ici que les résultats avec le noyau de Henderson^[
Il est en effet difficile de comparer proprement les résultats entre les différents noyaux car le filtre symétrique n'est pas le même. Cela a pour conséquence que des points de retournement peuvent être détectés. Par exemple, pour le filtre LC, sur les trois séries ayant une variabilité moyenne, seul 1 point de retournement est correctement détecté par l'ensemble des noyaux.
Toutefois, une première analyse des résultats montrent que les différents noyaux ont des performances proches en termes de déphasage et de révisions, sauf le noyau uniforme qui produit de moins bons résultats.
]. 

Pour le choix des poids dans l'approche FST, l'idée retenue dans cette étude est de faire un quadrillage du plan $[0,1]^3$ avec un pas de 0,05 et en imposant $\alpha + \beta + \gamma = 1$^[
Comme il n'est pas possible d'avoir un poids associé à la *timeliness* ($\gamma$) égale à 1 (sinon la fonction objectif n'est pas strictement convexe), on construit également un filtre avec un poids très proche de 1 ($1-1/1000$).
]. 
Pour chaque combinaison de poids, quatre ensembles de moyennes mobiles sont construits en forçant dans la minimisation la préservation de polynômes de degré 0 à 3. 
Le filtre symétrique utilisé est toujours celui de Henderson. 
Le degré de préservation polynomiale et l'ensemble de poids retenus sont ceux minimisant (en moyenne) le déphasage sur les séries simulées : pour l'ensemble des degrés de liberté, il s'agit toujours du filtre préservant les polynômes de degré 2 avec $\alpha = 0,00$ (*fidelity*), $\beta  =0,05$ (*smoothness*) et $\gamma = 0,95$ (*timeliness)*.

En excluant pour l'instant les paramétrisations locales des filtres polynomiaux, ce sont les filtres FST et LC semblent donner les meilleurs résultats en termes de déphasage dans la détection des points de retournement (figure \@ref(fig:graphstpsimul)). 
Les performances sont relativement proches de celles obtenues en prolongeant la série grâce à un modèle ARIMA.
Toutefois, lorsque la variabilité est faible, le filtre LC semble donner de moins bons résultats et c'est le filtre QL qui semble donner les meilleurs résultats.
Étonnement, c'est le filtre $b_{q,\varphi}$ qui minimise le déphasage qui donne les moins bons résultats.
Les autres filtres RKHS ont également une grande variabilité en termes de déphasage. 
Cela peut s'expliquer par le fait que la courbe des coefficients des moyennes mobiles asymétriques sont assez éloignées des coefficients du filtre symétrique : il y a donc potentiellement beaucoup de révisions dans la détection des points de retournement. 
En effet, lorsque le déphasage est défini comme la première date à laquelle le bon point de retournement est détecté, c'est le filtre $b_{q,G}$ qui donne les meilleurs résultats.

Pour les séries à variabilité moyenne, la paramétrisation locale des filtres LC et QL permet de réduire le déphasage.
Pour les séries à variabilité forte, le déphasage est uniquement réduit en utilisant les paramètres finaux $\hat\delta$ : l'estimation en temps réel semble ajouter plus de variabilité.
Pour les séries à variabilité forte, les performances semblent légèrement améliorées uniquement avec le filtre LC.

```{r graphstpsimul, echo=FALSE, out.width="100%", fig.cap="Distribution des déphasages sur les séries simulées."}
img <- sprintf("img/simulations/phase_shift_simul.%s", fig.ext)
knitr::include_graphics(img)
```

Concernant les révisions, la variabilité de la série a peu d'impact sur les performances respectives des différentes méthodes mais joue sur les ordres de grandeurs, c'est pourquoi les résultats ne sont présentés qu'avec les séries à variabilité moyenne (tableau \@ref(tab:simulrev))
Globalement, les filtres LC minimisent toujours les révisions (avec globalement peu d'impact de la paramétrisation locale des filtres) et les révisions sont plus importantes avec les filtres CQ, DAF et les filtres RKHS autres que $b_{q,\varphi}$.
Par ailleurs, c'est le filtre $b_{q,\varphi}$ qui conduit à le plus de révision entre l'avant-dernière et la dernière estimation, pour les mêmes raisons que celles évoquées dans le paragraphe précédent.   
Pour le filtre QL, il y a une forte révision entre la deuxième et la troisième estimation : cela peut venir du fait que pour la deuxième estimation (lorsque l'on connait un point dans le futur), le filtre QL associe un poids plus important à l'estimation en $t+1$ qu'à l'estimation en $t$, ce qui crée une discontinuité. 
Cette révision est fortement réduite avec la paramétrisation locale du filtre.
Pour les filtres polynomiaux autres que le filtre LC, les révisions importantes à la première estimation étaient prévisibles au vu de la courbe des coefficients : un poids très important est associé à l'observation courante et il y une forte discontinuité entre la moyenne mobile utilisée pour l'estimation en temps réelle (lorsqu'aucun point dans le futur n'est connu) et les autres moyennes mobiles.   
Enfin, pour le filtre issu de la méthode FST, la première estimation est fortement révisé, ce qui pouvait être attendu au vu de l'analyse de la courbe de coefficients (figure \@ref(fig:graphsfst)).
Ainsi, pour cette méthode, utiliser le même ensemble de poids ($\alpha$, $\beta$ et $\gamma$) pour construire l'ensemble des moyennes mobiles asymétriques ne semble pas pertinent : pour le filtre utilisé en temps réel (lorsqu'aucun point dans le futur est connu), on pourrait préférer donner un poids plus important à la fidélité afin de minimiser ces révisions.

Le prolongement de la série par un modèle ARIMA donnent révisions avec les dernières estimations du même ordre de grandeur que le filtre LC mais des révisions plus importantes entre les estimations consécutives (on pouvait s'y attendre comme souligné dans la section \@ref(subec:mmetprev)).

```{r simulrev, echo = FALSE}
rev_table <- readRDS("data/simulations_revisions.RDS")
title = "Moyenne des écarts relatifs des révisions pour les différents filtres sur les séries à variabilité moyenne."
pack_row_index = c("$MAE_{fe}(q) = \\mathbb E\\\\left[\\\\left|(y_{t|t+q} -  y_{t|last})/y_{t|last}\\\\right|\\\\right]$" = nrow(rev_table) / 2,
                   "$MAE_{ce}(q)=\\mathbb E\\\\left[
\\\\left|(y_{t|t+q} - y_{t|t+q+1})/y_{t|t+q+1}\\\\right|
\\\\right]$" = nrow(rev_table) / 2)
if(is_html){
  names(pack_row_index) <- gsub("\\\\","\\",names(pack_row_index), fixed = TRUE)
}
rev_table  %>%
  kable(format.args = list(digits = 2,
                           decimal.mark = ","),
        align = "c", booktabs = T, row.names = FALSE, 
        escape = FALSE, caption = title) %>%  
  kable_styling(latex_options=c(#"striped",  
    "hold_position")) %>% 
  pack_rows(index = pack_row_index, escape = FALSE)
```



## Série réelle

Les différentes méthodes sont également comparées sur deux exemples issus de la base FRED-MD @fredmd contenant des séries économiques sur les États-Unis^[
Les séries étudiées correspondent à la base publiée en novembre 2022.
] :

- autour du point de retournement de février 2001 sur le niveau d'emploi aux États-Unis (série `CE160V`, utilisée en logarithme) ;

- autour du point de retournement de novembre 2007 sur les ventes au détail des États-Unis (série `RETAILx`, utilisée en logarithme).

Ces séries ont une variabilité moyenne^[
La variabilité est déterminée en étudiant les séries jusqu'en janvier 2020.
] (un filtre symétrique de 13 termes est donc adapté).
Les figures \@ref(fig:ce16ovlp) à \@ref(fig:retailxautres) montrent les estimations successives de la tendance-cycle avec les différentes méthodes étudiées.


```{r ce16ovlp, echo=FALSE, out.width="90%", fig.cap="Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales."}
series <- "CE16OV"
date <- "fev2001"
img <- sprintf("img/nber/%s_%s_%s.%s",
               tolower(series),
               date,
               "lp",
               fig.ext)
knitr::include_graphics(img)
```

```{r ce16ovautres, echo=FALSE, out.width="90%", fig.cap="Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS, la méthode FST et en prolongeant la série par modèle ARIMA."}
img <- sprintf("img/nber/%s_%s_%s.%s",
               tolower(series),
               date,
               "autres",
               fig.ext)
knitr::include_graphics(img)
```

```{r retailxlp, echo=FALSE, out.width="90%", fig.cap="Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales."}
series <- "RETAILx"
date <- "nov2007"
img <- sprintf("img/nber/%s_%s_%s.%s",
               tolower(series),
               date,
               "lp",
               fig.ext)
knitr::include_graphics(img)
```

```{r retailxautres, echo=FALSE, out.width="90%", fig.cap="Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS, la méthode FST et en prolongeant la série par modèle ARIMA."}
img <- sprintf("img/nber/%s_%s_%s.%s",
               tolower(series),
               date,
               "autres",
               fig.ext)
knitr::include_graphics(img)
```



Même si presque toutes les méthodes ont un déphasage d'au plus 3 mois sur ce point de retournement, les estimations intermédiaires différent grandement. 
Il y a par exemple nettement plus de variabilités dans les estimations en temps réel pour les méthode QL, CQ et DAF et les filtres FST (figures \@ref(fig:retailxlp),  \@ref(fig:retailxfst1) et \@ref(fig:retailxfst2)), avec des estimations intermédiaires qui semblent peu plausibles (notamment pour la première estimation du mois d'avril 2020). Les premières estimations avec le filtre LC ne capte pas la remontée en mai et juin 2020 : cela peut provenir du fait qu'en fin de série, la tendance modélisée est linéaire.
Concernant les filtres RKHS, les estimations intermédiaires du filtre $b_{q,\varphi}$ semblent très erratiques, ce qui s'explique, encore une fois, par le fait que les moyennes mobiles asymétriques utilisées lorsque l'on se rapproche du cas symétrique sont éloignées de la moyenne mobile symétrique d'Henderson.
En revanche, les filtres $b_{q,\Gamma}$ et $b_{q,G}$ donnent des estimations intermédiaires proches de l'estimation finale, tout en minimisant le déphasage.

La qualité des estimations intermédiaires peut également être analysée grâces aux prévisions implicites des différentes méthodes. Pour rappel, il s'agit des prévisions de la série brute qui, en appliquant le filtre symétrique de Henderson sur la série prolongée, donne les mêmes estimations que les moyennes mobiles asymétriques. L'annexe \@ref(an-implicitforecasts) rassemble les différentes prévisions implicites des filtres étudiés dans cette section. 
Les prévisions implicites des filtres polynomiaux autres que LC, des filtres FST et du filtre $b_{q,\varphi}$ sont très peu plausibles et très éloignés des valeurs futures.
Le contrecoup en mai 2020 suite à la baisse d'avril 2020 ne sont pas prévus par le modèle ARIMA mais le sont par les filtres LC et RKHS, en revanche, les prévisions à l'horizon de 6 mois peuvent être assez éloignées des valeurs attendues.



```{r ce16ov-previmp-lp, echo=FALSE, out.width="90%", fig.cap="Prévisions implicites liées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales."}
series <- "CE16OV"
date <- "fev2001"
img <- sprintf("img/nber/%s_%s_prev_imp_%s.%s",
               tolower(series),
               date,
               "lp",
               fig.ext)
knitr::include_graphics(img)
```

```{r ce16ov-previmp-autres, echo=FALSE, out.width="90%", fig.cap="Prévisions implicites liées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS, la méthode FST et en prolongeant la série par modèle ARIMA."}
img <- sprintf("img/nber/%s_%s_prev_imp_%s.%s",
               tolower(series),
               date,
               "autres",
               fig.ext)
knitr::include_graphics(img)
```

```{r retailx-previmp-lp, echo=FALSE, out.width="90%", fig.cap="Prévisions implicites liées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales."}
series <- "RETAILx"
date <- "nov2007"
img <- sprintf("img/nber/%s_%s_prev_imp_%s.%s",
               tolower(series),
               date,
               "lp",
               fig.ext)
knitr::include_graphics(img)
```

```{r retailx-previmp-autres, echo=FALSE, out.width="90%", fig.cap="Prévisions implicites liées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS, la méthode FST et en prolongeant la série par modèle ARIMA."}
img <- sprintf("img/nber/%s_%s_prev_imp_%s.%s",
               tolower(series),
               date,
               "autres",
               fig.ext)
knitr::include_graphics(img)
```




\newpage
